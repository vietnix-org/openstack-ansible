---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

###
### This file contains commonly used overrides for convenience. Please inspect
### the defaults for each role to find additional override options.
###

## Debug and Verbose options.
debug: false

## Set the service setup host
# The default is to use localhost (the deploy host where ansible runs),
# but any other host can be used. If using an alternative host with all
# required libraries in a venv (eg: the utility container) then the
# python interpreter needs to be set. If it is not, the default is to
# the system python interpreter.
# If you wish to use the first utility container in the inventory for
# all service setup tasks, uncomment the following.
#
#openstack_service_setup_host: "{{ groups['utility_all'][0] }}"
#openstack_service_setup_host_python_interpreter: "/openstack/venvs/utility-{{ openstack_release }}/bin/python"

## Installation method for OpenStack services
# Default option (source) is to install the OpenStack services using PIP
# packages. An alternative method (distro) is to use the distribution cloud
# repositories to install OpenStack using distribution packages
install_method: source

## Common Glance Overrides
# Set glance_default_store to "swift" if using Cloud Files backend
# or "rbd" if using ceph backend; the latter will trigger ceph to get
# installed on glance. If using a file store, a shared file store is
# recommended. See the OpenStack-Ansible install guide and the OpenStack
# documentation for more details.
# Note that "swift" is automatically set as the default back-end if there
# are any swift hosts in the environment. Use this setting to override
# this automation if you wish for a different default back-end.
glance_default_store: rbd

## Ceph pool name for Glance to use
glance_rbd_store_pool: images
glance_rbd_store_chunk_size: 8

## Common Nova Overrides

# If you wish to change the dhcp_domain configured for both nova and neutron
# dhcp_domain: openstacklocal

## Common Glance Overrides when using a Swift back-end
# By default when 'glance_default_store' is set to 'swift' the playbooks will
# expect to use the Swift back-end that is configured in the same inventory.
# If the Swift back-end is not in the same inventory (ie it is already setup
# through some other means) then these settings should be used.
#
# NOTE: Ensure that the auth version matches your authentication endpoint.
#
# NOTE: If the password for glance_swift_store_key contains a dollar sign ($),
# it must be escaped with an additional dollar sign ($$), not a backslash. For
# example, a password of "super$ecure" would need to be entered as
# "super$$ecure" below.  See Launchpad Bug #1259729 for more details.
#
# glance_swift_store_auth_version: 3
# glance_swift_store_auth_address: "https://some.auth.url.com"
# glance_swift_store_user: "OPENSTACK_TENANT_ID:OPENSTACK_USER_NAME"
# glance_swift_store_key: "OPENSTACK_USER_PASSWORD"
# glance_swift_store_container: "NAME_OF_SWIFT_CONTAINER"
# glance_swift_store_region: "NAME_OF_REGION"

## Common Ceph Overrides
ceph_stable_release: octopus
cephx: true

ceph_mons:
  - 172.29.244.41
 
# Ceph client usernames for glance, cinder+nova and gnocchi
glance_ceph_client: glance
cinder_ceph_client: cinder
#manila_ceph_client: manila
cinder_backup_ceph_client: cinder-backup
#gnocchi_ceph_client: gnocchi

ceph_keyrings_dir: "/etc/openstack_deploy/ceph-keyrings"

## Custom Ceph Configuration File (ceph.conf)
# By default, your deployment host will connect to one of the mons defined above to
# obtain a copy of your cluster's ceph.conf.  If you prefer, uncomment ceph_conf_file
# and customise to avoid ceph.conf being copied from a mon.
ceph_conf_file: |
  [global]
  fsid = 3a1206ef-53f9-46dd-a0ba-fc534012c0b8
  #mon_initial_members = mon01-dev
  mon_host = [v2:172.29.244.41:3300,v1:172.29.244.41:6789]
  # optionally, you can use this construct to avoid defining this list twice:
  # mon_host = {{ ceph_mons|join(',') }}
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx


## Example environment variable setup:
## This is used by apt-cacher-ng to download apt packages:
# proxy_env_url: http://username:pa$$w0rd@10.10.10.9:9000/

## (1) This sets up a permanent environment, used during and after deployment:
# no_proxy_env: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['all_containers'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
# global_environment_variables:
#   HTTP_PROXY: "{{ proxy_env_url }}"
#   HTTPS_PROXY: "{{ proxy_env_url }}"
#   NO_PROXY: "{{ no_proxy_env }}"
#   http_proxy: "{{ proxy_env_url }}"
#   https_proxy: "{{ proxy_env_url }}"
#   no_proxy: "{{ no_proxy_env }}"
#
## (2) This is applied only during deployment, nothing is left after deployment is complete:
# deployment_environment_variables:
#   http_proxy: "{{ proxy_env_url }}"
#   https_proxy: "{{ proxy_env_url }}"
#   no_proxy: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['keystone_all'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"


## SSH connection wait time
# If an increased delay for the ssh connection check is desired,
# uncomment this variable and set it appropriately.
#ssh_delay: 5

###########
# per role settings:
###########

###### Role: openstack_hosts ######

## kernel modules for specific group hosts
#openstack_host_specific_kernel_modules: []

# Overridable package list is composed of the old override
# named user_package_list and the standard defaults _package_list
# openstack_hosts_package_list: "{{ _package_list + (user_package_list | default([])) }}"
# --> user_package_list:

# Set the maximum size of the connection tracking table.
openstack_host_nf_conntrack_max: 2000000

# Optional user defined list of sysctl options in the same dict item format 
openstack_user_kernel_options:
  - { key: 'net.ipv6.conf.all.disable_ipv6', value: "1" }
  - { key: 'net.ipv6.conf.default.disable_ipv6', value: "1" }
  - { key: 'net.ipv6.conf.lo.disable_ipv6', value: "1" }

# User defined list of extra packages to install on all hosts and containers
openstack_host_extra_distro_packages:
  - tcpdump
  - vim
  - lsof
  - strace
  - git
  - wget
  - curl
  - conntrack
  - screen
  - tree
  - nmap
  - rsync
  - hping3
  - vnstat
  - iperf3

# User defined list of extra packages to install on the host
#openstack_host_extra_metal_distro_packages: []

# Set the openstack domain name
openstack_domain: vietnix.vn

#user supplied list of CA certificates to copy to hosts from the deploy host
# example:
# - name: SnakeOilCorp.crt                             #the filename created on the target host (must be .crt on Ubuntu)
#    src: /etc/ssl/certs/snake-oil-cert-latest.pem     #the source file on the deploy host
#openstack_host_ca_certificates: []

###### Role: ansible-hardening #####

# nothing

###### Role: lxc_hosts ######

# Define a list of extra distribuition packages to install onto the hos# at the discretion of the deployer
#lxc_hosts_extra_distro_packages: []

# The container backing store can be set to 'overlayfs' to use overlayfs
# This should only be done for production use with a linux kernel > 3.14
# which is when overlayfs was merged into the mainline kernel
# lxc_container_backing_store: overlayfs

# The container backing method can be set to 'copy-on-write' to use LVM
# snapshot-backed containers when the container backing store is set to
# 'lvm'.
# lxc_container_backing_method: copy-on-write

###### Role: lxc_container_create ######

# container_fs.* is only used with building on an LVM backend
#lxc_container_fs_size: 5G
#lxc_container_fs_type: ext4

# Enable destroying then recreating containers
#lxc_container_recreate: false

# Enable running the veth wiring script
#lxc_container_veth_wiring: false

# Enable systemd-resolved
#lxc_container_enable_resolved: true

###### Role: unbound ######

##unbound_package_state: latest
#
#unbound_upstream_resolvers:
#  - 8.8.8.8
#  - 8.8.4.4
#  - 4.2.2.1
#  - 4.2.2.2
#unbound_localzone_mode: static
#unbound_root_zone: openstack
#unbound_regional_zone: "{{ service_region }}.{{ unbound_root_zone }}"
#unbound_listen_interface: 0.0.0.0
#unbound_access_control:
#  - cidr: 0.0.0.0/0
#    action: allow
#
## bind ipv6 socket
#unbound_ipv6: true
#
## deploys to server like priority-template.conf
## so for example, 000-server.conf in unbound_conf_dir which is in the include path by default
#unbound_configuration_templates:
#  - template: "server.conf"
#    priority: 0
#  - template: "openstack.conf"
#
## additional records example:
## unbound_records:
##   - name: 'extra'
##     priority: 50 (optional)
##     records:
##       - { host: hostname1.openstack, val: 10.10.10.1, type: A }
##       - { host: hostname2.openstack, val: 10.10.10.2, type: A }
#unbound_records: []

###### Role: repo_server ######

# nothing

###### Role: openstack_hosts ######

haproxy_stats_enabled: true
haproxy_stats_bind_address: 192.168.0.220
haproxy_username: admin
haproxy_stats_port: 1936
haproxy_bind_on_non_local: true

## haproxy SSL
haproxy_ssl: true
haproxy_ssl_all_vips: false
haproxy_ssl_dh_param: 2048
haproxy_ssl_self_signed_regen: no

# activate letsencrypt option
haproxy_ssl_letsencrypt_enable: false

haproxy_maxconn: 60000

# Add extra VIPs to all services
#extra_lb_vip_addresses: []

# HAproxy and keepalived
haproxy_use_keepalived: true
keepalived_use_latest_stable: true
haproxy_keepalived_external_vip_cidr: "192.168.0.250/24"
haproxy_keepalived_internal_vip_cidr: "172.29.236.250/22"
haproxy_keepalived_external_interface: "eno4"
haproxy_keepalived_internal_interface: "br-mgmt"

keepalived_ping_address: "192.168.0.1"

###### Role: memcache_server ######

memcached_connections: 32768

###### Role: galera_server ######

###### Role: qdrouterd ######


###### Role: rabbitmq_server ######
# Configure rabbitmq plugins
# This should be a comma-separated list of plugin names.
# Any plugin not listed will be disabled automatically.
# rabbitmq_plugins:
#   - name: rabbitmq_management,rabbitmq_prometheus
#     state: enabled


# By default, openstack-ansible configures all OpenStack services to talk to
# RabbitMQ over encrypted connections on port 5671. To opt-out of this default,
# set the rabbitmq_use_ssl variable to 'false'. The default setting of 'true'
# is highly recommended for securing the contents of RabbitMQ messages.
rabbitmq_use_ssl: false

# RabbitMQ management plugin is enabled by default, the guest user has been
# removed for security reasons and a new userid 'monitoring' has been created
# with the 'monitoring' user tag. In order to modify the userid, uncomment the
# following and change 'monitoring' to your userid of choice.
rabbitmq_monitoring_userid: monitoring

###### Role: os_keystone ######

## Caching
# This is a list of strings, each string contains a cache server's
# information (IP:port for example)
# The cache_servers default backend is memcached, so this variable
# should point to a list of memcached servers.
# If empty, caching is disabled.
#keystone_cache_servers: []

## LDAP Section
# Define Keystone LDAP domain configuration here.
# This may be used to add configuration for a LDAP identity back-end.
# See the http://docs.openstack.org/admin-guide/identity-integrate-with-ldap.html
#
# Each top-level entry is a domain name. Each entry below that are key: value pairs for
# the ldap section in the domain-specific configuration file.
#
# (EXAMPLE LAYOUT)
# keystone_ldap:
#   Users:
#     url: "ldap://127.0.0.1"
#     user: "root"
#     password: "secrete"
#     ...

## Extra HTTP headers for Keystone
# Add any additional headers here that Keystone should return.
#
# Example:
#
#   keystone_extra_headers:
#     - parameter: "Access-Control-Expose-Headers"
#       value: "X-Subject-Token"
#     - parameter: "Access-Control-Allow-Headers"
#       value: "Content-Type, X-Auth-Token"
#     - parameter: "Access-Control-Allow-Origin"
#       value: "*"
#keystone_extra_headers: []

# List of trusted IPs which can pass X-Forwarded-For
#keystone_set_real_ip_from: []


# Toggle whether memcache should be flushed when doing
# database migrations. This is sometimes useful when
# doing upgrades, but should not usually be required.
# ref: https://bugs.launchpad.net/openstack-ansible/+bug/1793389
#keystone_flush_memcache: no

###### Role: os_glance ######

###### Role: os_cinder ######
cinder_service_backup_program_enabled: True
cinder_service_backup_driver: cinder.backup.drivers.ceph.CephBackupDriver
cinder_service_backup_ceph_user: cinder-backup
cinder_service_backup_ceph_pool: backups
###### Role: os_nova ######

nova_nested_virt_enabled: False

nova_libvirt_inject_password: True

nova_libvirt_images_rbd_pool: vms

nova_libvirt_hw_disk_discard: 'unmap'
nova_libvirt_disk_cachemodes: 'network=writeback'

###### Role: openstack_neutron ######

# Other plugins can be added to the system by simply extending the list `neutron_plugin_base`.
# neutron_plugin_base:
#   - router
#   - firewall/firewall_v2 either one or the other, not both
#   - neutron_dynamic_routing.services.bgp.bgp_plugin.BgpPlugin
#   - vpnaas
#   - metering
#   - qos
#   - dns
#   - port_forwarding
neutron_plugin_base:
  - metering
  - qos

neutron_plugin_types:
  - ml2.sriov

# 
# ml2 network type drivers to load
#neutron_ml2_drivers_type: "flat,vlan,vxlan,local"
neutron_ml2_drivers_type: "flat,vlan"

neutron_vxlan_enabled: False

###### Role: openstack_horizon ######

# If nova_libvirt_inject_password is set to True, then this can also be enabled:
horizon_can_set_password: True
horizon_enable_cinder_backup: True
# Enables IPv6 support in Horizon, such as managing network subnets
horizon_enable_ipv6: True
# Enables router support in Horizon, disable if you don't have Neutron L3 agent
horizon_enable_router: False

# WSGI tuning parameters
# horizon_wsgi_processes: 4
# horizon_wsgi_threads: 4

## Cap the maximun number of threads / workers when a user value is unspecified.
horizon_wsgi_threads_max: 16


## Horizon SSL
horizon_ssl_cert: /etc/ssl/certs/horizon.pem
horizon_ssl_key: /etc/ssl/private/horizon.key
horizon_ssl_ca_cert: /etc/ssl/certs/horizon-ca.pem

# Set these variables to deploy custom certificates
# horizon_user_ssl_cert: <path to cert on ansible deployment host>
# horizon_user_ssl_key: <path to cert on ansible deployment host>
# horizon_user_ssl_ca_cert: <path to cert on ansible deployment host>


###### Role: openstack_ ######
###### Role: openstack_ ######
#
#
